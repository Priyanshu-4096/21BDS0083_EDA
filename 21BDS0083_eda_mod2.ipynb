{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOJUlaBEzYeXsl0lNBnOuV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshu-4096/21BDS0083_EDA/blob/main/21BDS0083_eda_mod2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Priyanshu-4096/21BDS0083_EDA"
      ],
      "metadata": {
        "id": "YWqEiaVU5L3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Name - Priyanshu Joarder\n",
        "#Reg no - 21BDS0083\n",
        "#Course Title - Exploratory Data Analysis\n",
        "#Course Code - BCSE331L"
      ],
      "metadata": {
        "id": "Meue-fgs5RSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 2: Data Transformation"
      ],
      "metadata": {
        "id": "UQ_v6zZCwgdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnmD4itC4tqv",
        "outputId": "04eea1ae-43f5-4989-937c-c530a302b6fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate rows before deduplication: 0\n",
            "Duplicate rows after deduplication: 0\n",
            "\n",
            "Missing values per column:\n",
            "rownames        0\n",
            "Person_ID       0\n",
            "Hospitalised    0\n",
            "Died            0\n",
            "Urban           0\n",
            "Year            0\n",
            "Month           0\n",
            "Sex             0\n",
            "Age             0\n",
            "Education       0\n",
            "Occupation      0\n",
            "method          0\n",
            "Age_group       0\n",
            "dtype: int64\n",
            "\n",
            "Missing values after imputation:\n",
            "rownames        0\n",
            "Person_ID       0\n",
            "Hospitalised    0\n",
            "Died            0\n",
            "Urban           0\n",
            "Year            0\n",
            "Month           0\n",
            "Sex             0\n",
            "Age             0\n",
            "Education       0\n",
            "Occupation      0\n",
            "method          0\n",
            "Age_group       0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-8388480c3057>:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['Hospitalised'] = df['Hospitalised'].replace('yes', 1).replace('no', 0)\n",
            "<ipython-input-2-8388480c3057>:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['Died'] = df['Died'].replace('yes', 1).replace('no', 0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('SuicideChina.csv')\n",
        "\n",
        "# Data deduplication\n",
        "print(\"Duplicate rows before deduplication:\", df.duplicated().sum())\n",
        "df = df.drop_duplicates()\n",
        "print(\"Duplicate rows after deduplication:\", df.duplicated().sum())\n",
        "\n",
        "# Replacing values\n",
        "df['Hospitalised'] = df['Hospitalised'].replace('yes', 1).replace('no', 0)\n",
        "df['Died'] = df['Died'].replace('yes', 1).replace('no', 0)\n",
        "df['Urban'] = df['Urban'].replace('yes', 1).replace('no', 0)\n",
        "\n",
        "# Discretization and binning\n",
        "df['Age_group'] = pd.cut(df['Age'], bins=[-1, 20, 40, 60, 80, 100], labels=['0-19', '20-39', '40-59', '60-79', '80+'])\n",
        "\n",
        "# Handling missing data\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Traditional method - Mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df[['Age']] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Maximum Likelihood Estimation (MLE) imputation (for comparison)\n",
        "imputer_mle = IterativeImputer(max_iter=10, random_state=42)\n",
        "df[['Age']] = imputer_mle.fit_transform(df[['Age']])\n",
        "\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Save the transformed dataset\n",
        "df.to_csv('SuicideChina_transformed.csv', index=False)"
      ]
    }
  ]
}